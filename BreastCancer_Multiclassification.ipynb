{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shasso2s/-Projekt-zur-Datenanalyse_Microbiome-Analyse-mittels-Python/blob/main/BreastCancer_Multiclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGO2wDZYs7zY"
      },
      "source": [
        "# Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XRpI6eXgtQ74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "20f8a5d5-9707-4b7b-a8f9-14209550ccb6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-299c7a38057b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptical_flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/models/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mshufflenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/models/quantization/mobilenet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantizableMobileNetV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmv2_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenetv3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantizableMobileNetV3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmobilenet_v3_large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmv3_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmv2_all\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmv3_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/models/quantization/mobilenetv2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantStub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeQuantStub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse_modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobilenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInvertedResidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMobileNetV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'QuantStub' from 'torch.ao.quantization' (/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import cv2\n",
        "import glob\n",
        "from PIL import Image, ImageOps\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "from torchvision.transforms import transforms\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from torch.autograd._functions import tensor\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn import preprocessing\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import  torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import torch.optim.lr_scheduler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "from google.colab import  drive\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import tensorboard\n",
        "import tensorflow\n",
        "\n",
        "\n",
        "writer =SummaryWriter(\"runs/mnist\")\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "     "
      ],
      "metadata": {
        "id": "ym4pKpPoe-S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBKpFVlD6TaJ"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEb6VGia7AmD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1CzUy4weKvL"
      },
      "source": [
        "manulally i have splitted my dataset into train , validatin and test(60% train,20% valid,20% test)\n",
        "* in the followin methode i have created a dataFrame for my training set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Qaabno1tD6m"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_train():\n",
        "        path_train=glob.glob('drive/MyDrive/Projekt_DatenAnalyse/BreakHis/BreakHis/Train/*/*/*/*.png')\n",
        "        \n",
        "        data = pd.DataFrame(index=np.arange(0,len(path_train)), columns=['patient_id','sublabel','path'])\n",
        "        patient_id=[]\n",
        "        sublabel=[]\n",
        "        for i in range(len(path_train)):\n",
        "                patient_id.append(path_train[i].split('/')[9].split('.')[0])\n",
        "                sublabel.append(path_train[i].split('/')[6])\n",
        "        data['patient_id']=patient_id\n",
        "        data['path']=path_train\n",
        "        data['sublabel']=sublabel\n",
        "        return data\n",
        "data_train=data_train()\n",
        "print(data_train.head(4))\n",
        "print('lenght of training set is {}'.format(len(data_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3gKH5iU7y2I"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_valid():\n",
        "        path_valid = glob.glob('drive/MyDrive/Projekt_DatenAnalyse/BreakHis/BreakHis/Valid/*/*/*.png', recursive=True)\n",
        "\n",
        "        data = pd.DataFrame(index=np.arange(0, len(path_valid )), columns=['patient_id', 'sublabel', 'path'])\n",
        "        patient_id = []\n",
        "        sublabel = []\n",
        "        for i in range(len(path_valid )):\n",
        "            patient_id.append(path_valid[i].split('/')[8].split('.')[0])\n",
        "            sublabel.append(path_valid [i].split('/')[6])\n",
        "        data['patient_id'] = patient_id\n",
        "        data['path'] = path_valid\n",
        "        data['sublabel'] = sublabel\n",
        "        return data\n",
        "data_valid=data_valid()\n",
        "print(data_valid.head(4))\n",
        "print('lenght of the validation set is {}'.format(len(data_valid)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHqt3wh5Imeo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_test():\n",
        "        path_test = glob.glob('drive/MyDrive/Projekt_DatenAnalyse/BreakHis/BreakHis/Test/*/*/*.png', recursive=True)\n",
        "        data = pd.DataFrame(index=np.arange(0, len(path_test)), columns=['patient_id', 'sublabel', 'path'])\n",
        "        patient_id = []\n",
        "        sublabel = []\n",
        "        for i in range(len(path_test)):\n",
        "            patient_id.append(path_test[i].split('/')[8].split('.')[0])\n",
        "            sublabel.append(path_test[i].split('/')[6])\n",
        "        data['patient_id'] = patient_id\n",
        "        data['path'] = path_test\n",
        "        data['sublabel'] = sublabel\n",
        "        return data\n",
        "data_test=data_test()\n",
        "print(data_test.head(4))\n",
        "print('lenght of test_set is {}'.format(len(data_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIBh4bulfTKi"
      },
      "source": [
        "for each dataframe i create json dataframe of it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYNCNchFMzCt"
      },
      "outputs": [],
      "source": [
        "#data_train.to_json('drive/MyDrive/Projekt_DatenAnalyse/dataframes/train_set.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7DbFNjHOgCI"
      },
      "outputs": [],
      "source": [
        "#data_valid.to_json('drive/MyDrive/Projekt_DatenAnalyse/dataframes/valid_set.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdI71wrzO0Qf"
      },
      "outputs": [],
      "source": [
        "#data_test.to_json('drive/MyDrive/Projekt_DatenAnalyse/dataframes/test_set.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iaRSDS7MjBG"
      },
      "outputs": [],
      "source": [
        "data_train=pd.read_json('drive/MyDrive/Projekt_DatenAnalyse/dataframes/training_set.json')\n",
        "print('how many subclass do i have:'.format(data_train['sublabel'].unique()))\n",
        "subclasses={1:'Adenosis',2 :'fibroadenoma', 3 :'phyllodes_tumor', 4 :'tubular_adenoma' ,5:'ductal_carcinoma' ,6 :'lobular_carcinoma',7:'mucinous_carcinoma', 8 :'papillary_carcinoma'}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAsO1i7pOpu3"
      },
      "outputs": [],
      "source": [
        "#transforms to apply to our data:\n",
        "def my_transform(key=\"train_transform\"):\n",
        "    train_transform = [transforms.Resize((120, 120)),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.RandomVerticalFlip(),\n",
        "                    transforms.RandomRotation(90), \n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.5643, 0.5112, 0.5667],std=[0.2115, 0.2073, 0.2088])]\n",
        "\n",
        "    val_transform = [transforms.Resize((120,120)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.5733, 0.2270, 0.7036],std=[1.1473, 1.0636, 1.2201])]\n",
        "    \n",
        "    test_transform = [transforms.Resize((120,120)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.5115, 0.2252, 0.6199],std=[1.1304, 1.0710, 1.1989])]\n",
        "        \n",
        "    data_transforms = {'train_transform': transforms.Compose(train_transform),\n",
        "                       'val_transform': transforms.Compose(val_transform),\n",
        "                       'test_transform': transforms.Compose(test_transform)}\n",
        "    return data_transforms[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbieDtM7PB2F"
      },
      "outputs": [],
      "source": [
        "train_set=torchvision.datasets.ImageFolder(root='drive/MyDrive/Projekt_DatenAnalyse/BreakHis/BreakHis/Train',transform=my_transform(key=\"train_transform\"))\n",
        "\n",
        "validation_set=torchvision.datasets.ImageFolder(root='drive/MyDrive/Projekt_DatenAnalyse/BreakHis/BreakHis/Valid',transform=my_transform(key=\"val_transform\"))\n",
        "\n",
        "test_set=torchvision.datasets.ImageFolder(root='drive/MyDrive/Projekt_DatenAnalyse/BreakHis/BreakHis/Test',transform=my_transform(key=\"test_transform\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CJ31XXER7Cd"
      },
      "outputs": [],
      "source": [
        "image_datasets = {\"train\": train_set, \"val\": validation_set, \"test\": test_set}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\", \"test\"]}\n",
        "print(dataset_sizes)\n",
        "class_names = image_datasets['train'].classes\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCQiOP11SdgU"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-X6dFbzSQUt"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-stdkJ_SkfZ"
      },
      "outputs": [],
      "source": [
        "dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader, \"test\": test_dataloader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W97y5ivASosG"
      },
      "outputs": [],
      "source": [
        "print(\"Our training dataset has the size: \" + str(dataset_sizes[\"train\"]))\n",
        "print(\"Divided by our batch_size of \" + str(BATCH_SIZE) + \" this leads to \" + \n",
        "      str(len(dataloaders[\"train\"])) + \" iterations for each epoch while training.\")\n",
        "print()\n",
        "print(\"Our validation data set has the size: \" + str(dataset_sizes[\"val\"]))\n",
        "print(\"Divided by our batch_size of \" + str(BATCH_SIZE) + \" this leads to \" + \n",
        "      str(len(dataloaders[\"val\"])) + \" iterations for each epoch for validation.\")\n",
        "print()\n",
        "print(\"Our test data set has the size: \" + str(dataset_sizes[\"test\"]))\n",
        "print(\"Divided by our batch_size of \" + str(BATCH_SIZE) + \" this leads to \" + \n",
        "      str(len(dataloaders[\"test\"])) + \" iterations for each epoch for testing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QCwQI6pUfgK"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.figure(figsize=(40, 40))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "def show_databatch(inputs, classes):\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "show_databatch(inputs, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5PQp5A5VF_t"
      },
      "outputs": [],
      "source": [
        "def visualize_model(vgg, num_images=8):\n",
        "    was_training = vgg.training\n",
        "    \n",
        "    # Set model for evaluation\n",
        "    vgg.train(False)\n",
        "    vgg.eval() \n",
        "    \n",
        "    images_so_far = 0\n",
        "\n",
        "    for i, data in enumerate(dataloaders['test']):\n",
        "        inputs, labels = data\n",
        "        size = inputs.size()[0]\n",
        "        \n",
        "        \n",
        "        outputs = vgg(inputs)\n",
        "        \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        predicted_labels = [preds[j] for j in range(inputs.size()[0])]\n",
        "        \n",
        "        \n",
        "        show_databatch(inputs.data.cpu(), labels.data.cpu())\n",
        "        \n",
        "        show_databatch(inputs.data.cpu(), predicted_labels)\n",
        "        \n",
        "        del inputs, labels, outputs, preds, predicted_labels\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        images_so_far += size\n",
        "        if images_so_far >= num_images:\n",
        "            break\n",
        "        \n",
        "    vgg.train(mode=was_training) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59EybP8cas8n"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gBn1yn4X_OU"
      },
      "outputs": [],
      "source": [
        "#device=torch.device('cuda' if torch.utils.is_available() else: 'cpu')\n",
        "device='cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOlBipHdVYGf"
      },
      "outputs": [],
      "source": [
        "def eval_model(vgg, criterion):\n",
        "    since = time.time()\n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    \n",
        "    test_batches = len(dataloaders['test'])\n",
        "    print(\"Evaluating model\")\n",
        "    print('-' * 10)\n",
        "    \n",
        "    for i, data in enumerate(dataloaders['test']):\n",
        "        #input=input.to(device)\n",
        "        #labels=labels.to(device)\n",
        "        if i % 100 == 0:\n",
        "            print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n",
        "\n",
        "        vgg.train(False)\n",
        "        vgg.eval()\n",
        "        inputs, labels = data\n",
        "\n",
        "        \n",
        "\n",
        "        outputs = vgg(inputs)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        print(outputs.shape)\n",
        "        print(inputs.shape)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss_test += loss.data\n",
        "        acc_test += torch.sum(preds == labels.data)\n",
        "\n",
        "        del inputs, labels, outputs, preds\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    avg_loss = loss_test / dataset_sizes['test']\n",
        "    avg_acc = acc_test / dataset_sizes['test']\n",
        "    \n",
        "    elapsed_time = time.time() - since\n",
        "    print()\n",
        "    print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
        "    print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n",
        "    print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n",
        "    print('-' * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2caCl9MVvtu"
      },
      "outputs": [],
      "source": [
        "vgg16 = models.vgg16_bn()\n",
        "#vgg16.load_state_dict(torch.load(\"../input/vgg16bn/vgg16_bn.pth\"))\n",
        "print(vgg16.classifier[6].out_features) # 1000 \n",
        "\n",
        "\n",
        "# Freeze training for all layers\n",
        "for param in vgg16.features.parameters():\n",
        "    param.require_grad = False\n",
        "\n",
        "# Newly created modules have require_grad=True by default\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "features = list(vgg16.classifier.children())[:-1] # Remove last layer\n",
        "features.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\n",
        "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "print(vgg16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "321mSGAPWNV6"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDyh9qInWWGm"
      },
      "outputs": [],
      "source": [
        "print(\"Test before training\")\n",
        "eval_model(vgg16, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NiNwTijMk88B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Saz5ddJyyyRZ"
      },
      "source": [
        "this part is just for testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.figure(figsize=(40, 40))\n",
        "    \n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(train_dataloader))\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out,title=[class_names[x] for x in classes])\n",
        "\n"
      ],
      "metadata": {
        "id": "WVV7DnQdjVKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model_vgg16 = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\n",
        "print(model_vgg16 .classifier)\n",
        "\n",
        "model_vgg16 .classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 4096, bias = True),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(4096, 2048, bias = True),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(2048, 8)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "JKZPtEJCl4BB",
        "outputId": "8480d062-398c-4371-ce01-f632541692da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/pytorch_vision_main/torchvision/io/image.py:12: UserWarning: Failed to load image Python extension: \n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cd0ea6620b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_vgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch/vision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vgg16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vgg16\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model_vgg16 .classifier = nn.Sequential(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mrepo_or_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_cache_or_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mhubconf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m     \u001b[0mhub_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhubconf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/hubconf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malexnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdensenet121\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensenet169\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensenet201\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensenet161\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from torchvision.models.efficientnet import (\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptical_flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/models/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mshufflenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/models/quantization/mobilenet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantizableMobileNetV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmv2_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenetv3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantizableMobileNetV3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmobilenet_v3_large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmv3_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmv2_all\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmv3_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_main/torchvision/models/quantization/mobilenetv2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantStub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeQuantStub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse_modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobilenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInvertedResidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMobileNetV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'QuantStub' from 'torch.ao.quantization' (/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#model_ft = model_ft.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "aqnaDJhcl_rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22zLspKftaPQ"
      },
      "outputs": [],
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
        "    correct_pred = (y_pred_tags == y_test).float()\n",
        "    acc = correct_pred.sum() / len(correct_pred)\n",
        "    acc = torch.round(acc * 100)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_stats = {\n",
        "    'train': [],\n",
        "    \"val\": []\n",
        "}\n",
        "loss_stats = {\n",
        "    'train': [],\n",
        "    \"val\": []\n",
        "}"
      ],
      "metadata": {
        "id": "qIyqA_WyS71S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "8xfKSrnLTU9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Begin training.\")\n",
        "for e in tqdm(range(1, 11)):\n",
        "    # TRAINING\n",
        "    train_epoch_loss = 0\n",
        "    train_epoch_acc = 0\n",
        "    model_vgg16.train()\n",
        "\n",
        "    for X_train_batch, y_train_batch in train_dataloader:\n",
        "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_train_pred = model_vgg16(X_train_batch).squeeze()\n",
        "        train_loss = criterion(y_train_pred, y_train_batch)\n",
        "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_epoch_loss += train_loss.item()\n",
        "        train_epoch_acc += train_acc.item()\n",
        "\n",
        "    # VALIDATION\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_epoch_loss = 0\n",
        "        val_epoch_acc = 0\n",
        "\n",
        "        for X_val_batch, y_val_batch in val_dataloader:\n",
        "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "\n",
        "            y_val_pred = model_vgg16 (X_val_batch).squeeze()\n",
        "            #y_val_pred = torch.unsqueeze(y_val_pred, 0)\n",
        "            val_loss = criterion(y_val_pred, y_val_batch)\n",
        "\n",
        "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
        "            val_epoch_loss += train_loss.item()\n",
        "            val_epoch_acc += train_acc.item()\n",
        "\n",
        "    loss_stats['train'].append(train_epoch_loss/len(train_dataloader))\n",
        "    loss_stats['val'].append(val_epoch_loss/len(val_dataloader))\n",
        "    accuracy_stats['train'].append(train_epoch_acc/len(train_dataloader))\n",
        "    accuracy_stats['val'].append(val_epoch_acc/len(val_dataloader))\n",
        "\n",
        "\n",
        "    print(f'Epoch {e+0:02}: | Train Loss: {train_epoch_loss/len(train_dataloader):.5f} | Val Loss: {val_epoch_loss/len(val_dataloader):.5f} | Train Acc: {train_epoch_acc/len(train_dataloader):.3f}| Val Acc: {val_epoch_acc/len(val_dataloader):.3f}')"
      ],
      "metadata": {
        "id": "3vBNpSFmS-51"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BreastCancer_Multiclassification.ipynb",
      "provenance": [],
      "mount_file_id": "1X56lQMjJRvq17UAch0TMHdLeQ68abI9H",
      "authorship_tag": "ABX9TyOqOmxtCsgkK7tg6uwVUW5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}